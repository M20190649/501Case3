{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 3 : Data Science in NYC Taxi and Uber Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* [Analyzing 1.1 Billion NYC Taxi and Uber Trips](http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/) \n",
    "* Please download the NYC taxi and Uber dataset from [here](https://github.com/toddwschneider/nyc-taxi-data).\n",
    "* [TED Talks](https://www.ted.com/talks) for examples of 7 minutes talks.\n",
    "\n",
    "\n",
    "**NOTE**\n",
    "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: pick a data science problem that you plan to solve using Uber/Taxi Data\n",
    "* The problem should be important and interesting, which has a potential impact in some area.\n",
    "* The problem should be solvable using the data and data science solutions.\n",
    "\n",
    "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Which areas in New York have the worst traffic? Is your choice of ride indicative of your political\n",
    "# affiliation? We were able to discover the answers to these questions--they may surprise you!\n",
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "jan = pd.read_csv(\"yellow_tripdata_2014-01.csv\")\n",
    "feb = pd.read_csv(\"yellow_tripdata_2014-02.csv\")\n",
    "mar = pd.read_csv(\"yellow_tripdata_2014-03.csv\")\n",
    "apr = pd.read_csv(\"yellow_tripdata_2014-04.csv\")\n",
    "may = pd.read_csv(\"yellow_tripdata_2014-05.csv\")\n",
    "june = pd.read_csv(\"yellow_tripdata_2014-06.csv\")\n",
    "july = pd.read_csv(\"yellow_tripdata_2014-07.csv\")\n",
    "aug = pd.read_csv(\"yellow_tripdata_2014-08.csv\")\n",
    "sept = pd.read_csv(\"yellow_tripdata_2014-09.csv\")\n",
    "octo = pd.read_csv(\"yellow_tripdata_2014-10.csv\")\n",
    "nov = pd.read_csv(\"yellow_tripdata_2014-11.csv\")\n",
    "dec = pd.read_csv(\"yellow_tripdata_2014-12.csv\")\n",
    "uber = pd.read_csv(\"uber-raw-data-janjune-15.csv\")\n",
    "\n",
    "\n",
    "# gotta delete rows which have 0 for pickup long, pickup lat, dropoff long, dropoff lat\n",
    "# couldn't get it working in a loop so I just did it for each month individually\n",
    "\n",
    "jan = jan[jan[' pickup_longitude'] != 0.0]\n",
    "jan = jan[jan[' pickup_latitude'] != 0.0]\n",
    "jan = jan[jan[' dropoff_longitude'] != 0.0]\n",
    "jan = jan[jan[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "feb = feb[feb[' pickup_longitude'] != 0.0]\n",
    "feb = feb[feb[' pickup_latitude'] != 0.0]\n",
    "feb = feb[feb[' dropoff_longitude'] != 0.0]\n",
    "feb = feb[feb[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "mar = mar[mar[' pickup_longitude'] != 0.0]\n",
    "mar = mar[mar[' pickup_latitude'] != 0.0]\n",
    "mar = mar[mar[' dropoff_longitude'] != 0.0]\n",
    "mar = mar[mar[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "apr = apr[apr[' pickup_longitude'] != 0.0]\n",
    "apr = apr[apr[' pickup_latitude'] != 0.0]\n",
    "apr = apr[apr[' dropoff_longitude'] != 0.0]\n",
    "apr = apr[apr[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "may = may[may[' pickup_longitude'] != 0.0]\n",
    "may = may[may[' pickup_latitude'] != 0.0]\n",
    "may = may[may[' dropoff_longitude'] != 0.0]\n",
    "may = may[may[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "june = june[june[' pickup_longitude'] != 0.0]\n",
    "june = june[june[' pickup_latitude'] != 0.0]\n",
    "june = june[june[' dropoff_longitude'] != 0.0]\n",
    "june = june[june[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "july = july[july[' pickup_longitude'] != 0.0]\n",
    "july = july[july[' pickup_latitude'] != 0.0]\n",
    "july = july[july[' dropoff_longitude'] != 0.0]\n",
    "july = july[july[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "aug = aug[aug[' pickup_longitude'] != 0.0]\n",
    "aug = aug[aug[' pickup_latitude'] != 0.0]\n",
    "aug = aug[aug[' dropoff_longitude'] != 0.0]\n",
    "aug = aug[aug[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "sept = sept[sept[' pickup_longitude'] != 0.0]\n",
    "sept = sept[sept[' pickup_latitude'] != 0.0]\n",
    "sept = sept[sept[' dropoff_longitude'] != 0.0]\n",
    "sept = sept[sept[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "octo = octo[octo[' pickup_longitude'] != 0.0]\n",
    "octo = octo[octo[' pickup_latitude'] != 0.0]\n",
    "octo = octo[octo[' dropoff_longitude'] != 0.0]\n",
    "octo = octo[octo[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "nov = nov[nov[' pickup_longitude'] != 0.0]\n",
    "nov = nov[nov[' pickup_latitude'] != 0.0]\n",
    "nov = nov[nov[' dropoff_longitude'] != 0.0]\n",
    "nov = nov[nov[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "dec = dec[dec[' pickup_longitude'] != 0.0]\n",
    "dec = dec[dec[' pickup_latitude'] != 0.0]\n",
    "dec = dec[dec[' dropoff_longitude'] != 0.0]\n",
    "dec = dec[dec[' dropoff_latitude'] != 0.0]\n",
    "\n",
    "\n",
    "jan.to_csv('jan_cleaned.csv')\n",
    "feb.to_csv('feb_cleaned.csv')\n",
    "mar.to_csv('mar_cleaned.csv')\n",
    "apr.to_csv('apr_cleaned.csv')\n",
    "may.to_csv('may_cleaned.csv')\n",
    "june.to_csv('june_cleaned.csv')\n",
    "july.to_csv('july_cleaned.csv')\n",
    "aug.to_csv('aug_cleaned.csv')\n",
    "sept.to_csv('sept_cleaned.csv')\n",
    "octo.to_csv('octo_cleaned.csv')\n",
    "nov.to_csv('nov_cleaned.csv')\n",
    "dec.to_csv('dec_cleaned.csv')\n",
    "\n",
    "\n",
    "\n",
    "jan_mode = jan.mode(axis=0)\n",
    "feb_mode = feb.mode(axis=0)\n",
    "mar_mode = mar.mode(axis=0)\n",
    "apr_mode = apr.mode(axis=0)\n",
    "may_mode = may.mode(axis=0)\n",
    "june_mode = june.mode(axis=0)\n",
    "july_mode = july.mode(axis=0)\n",
    "aug_mode = aug.mode(axis=0)\n",
    "sept_mode = sept.mode(axis=0)\n",
    "octo_mode = octo.mode(axis=0)\n",
    "nov_mode = nov.mode(axis=0)\n",
    "dec_mode = dec.mode(axis=0)\n",
    "\n",
    "# June mode has 3 rows because there are three days which occur the same number of times in the data set\n",
    "# We drop the second and third rows, since they don't contain latitude and longitude information\n",
    "june_mode=june_mode.drop([1,2])\n",
    "jan_mode.to_csv('jan_mode.csv')\n",
    "feb_mode.to_csv('feb_mode.csv')\n",
    "mar_mode.to_csv('mar_mode.csv')\n",
    "apr_mode.to_csv('apr_mode.csv')\n",
    "may_mode.to_csv('may_mode.csv')\n",
    "june_mode.to_csv('june_mode.csv')\n",
    "july_mode.to_csv('july_mode.csv')\n",
    "aug_mode.to_csv('aug_mode.csv')\n",
    "sept_mode.to_csv('sept_mode.csv')\n",
    "octo_mode.to_csv('octo_mode.csv')\n",
    "nov_mode.to_csv('nov_mode.csv')\n",
    "dec_mode.to_csv('dec_mode.csv')\n",
    "\n",
    "\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#print(uber.columns.values)\n",
    "uber = uber.sort_values(by=['locationID'])\n",
    "uber = uber.sample(5000)\n",
    "counts = uber['locationID'].value_counts()\n",
    "plt.scatter(counts.index,counts.values)\n",
    "plt.title('Distribution of Uber pickup locations')\n",
    "plt.xlabel('Pickup ID')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection/Processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Exploring the Uber/Taxi Dataset\n",
    "\n",
    "**plot the spatial distribution of the pickup locations of 5000 Uber trips** \n",
    "* collect a set of 5000 Uber trips\n",
    "* plot the distribution of the pickup locations using a scatter plot figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "# Where in New York City will have the worst traffic?\n",
    "# Of course, it will be the location with the most vehicles.\n",
    "# To get an estimate of the location with the most vehicles, we will analyze the data from 2014 and find the most frequent \n",
    "# pickup and dropoff locations for each month.\n",
    "# This will show which areas to avoid each month\n",
    "# Then, we take one month and separate it by day of the week.\n",
    "# We then run the same analysis to see if different areas are traveled depending on the day of the week.\n",
    "\n",
    "\n",
    "\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "jan_mode = pd.read_csv(\"jan_mode.csv\")\n",
    "feb_mode = pd.read_csv(\"feb_mode.csv\")\n",
    "mar_mode = pd.read_csv(\"mar_mode.csv\")\n",
    "apr_mode = pd.read_csv(\"apr_mode.csv\")\n",
    "may_mode = pd.read_csv(\"may_mode.csv\")\n",
    "june_mode = pd.read_csv(\"june_mode.csv\")\n",
    "july_mode = pd.read_csv(\"july_mode.csv\")\n",
    "aug_mode = pd.read_csv(\"aug_mode.csv\")\n",
    "sept_mode = pd.read_csv(\"sept_mode.csv\")\n",
    "octo_mode = pd.read_csv(\"octo_mode.csv\")\n",
    "nov_mode = pd.read_csv(\"nov_mode.csv\")\n",
    "dec_mode = pd.read_csv(\"dec_mode.csv\")\n",
    "\n",
    "\n",
    "# combining all the info into a single dataframe, then bringing it down to just latitudes and longitudes\n",
    "\n",
    "frames = [jan_mode, feb_mode, mar_mode, apr_mode, may_mode, june_mode, july_mode, aug_mode, sept_mode, octo_mode, \n",
    "          nov_mode, dec_mode]\n",
    "modes = pd.concat(frames)\n",
    "modes = modes.drop([' passenger_count', ' trip_distance', ' rate_code', ' store_and_fwd_flag', ' payment_type', ' fare_amount', \n",
    "            ' surcharge', ' mta_tax', ' tip_amount', ' tolls_amount', ' total_amount', 'vendor_id', ' pickup_datetime',\n",
    "            ' dropoff_datetime'], axis=1)\n",
    "modes = modes.reset_index()\n",
    "modes.index = modes.index + 1\n",
    "modes = modes.drop(['index', 'Unnamed: 0'], axis=1)\n",
    "modes\n",
    "\n",
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "# January pickup address: 2117 37th Avenue, Long Island City, NY 11101\n",
    "    # In 2014, this was the address of Endo Freight Forwarders,\n",
    "    # a cargo and freight company.\n",
    "    # However, it's also close to a bus stop.\n",
    "# January dropoff address: 226 East 67th Street, New York, NY 10065\n",
    "    # In 2014, this was the address of the 67th Street Garage\n",
    "\n",
    "    \n",
    "# February pickup address: 2117 37th Avenue, Long Island City, NY 11101\n",
    "    # In 2014, this was the address of Endo Freight Forwarders, close to a bus stop\n",
    "# February dropoff address: Roosevelt Island, Manhattan, NY 10044\n",
    "    # This is a small island located in Manhattan which has many rental\n",
    "    # locations: https://en.wikipedia.org/wiki/Roosevelt_Island\n",
    "\n",
    "    \n",
    "# March pickup address: 2117 37th Avenue, Long Island City, NY 11101\n",
    "    # In 2014, this was the address of Endo Freight Forwarders, close to a bus stop\n",
    "# March dropoff address: 4662 21st Street, Queens, NY 11101\n",
    "    # Believe it or not, this is the location of a subway stop\n",
    "    # I had to go onto Google streetview to find this information\n",
    "    # It's the 21st Street Station, served by the G train at all times\n",
    "    # https://en.wikipedia.org/wiki/21st_Street_station_(IND_Crosstown_Line)\n",
    "    \n",
    "\n",
    "# April pickup address: 2117 37th Avenue, Long Island City, NY 11101\n",
    "    # In 2014, this was the address of Endo Freight Forwarders, close to a bus stop\n",
    "# April dropoff address: 4662 21st Street, Queens, NY 11101\n",
    "     # 21st Street Subway Station\n",
    "\n",
    "    \n",
    "# May pickup address: 4738 5th Street, Long Island City, NY 11101\n",
    "    # This is the 47th Avenue Garage\n",
    "# May dropoff address: 4662 21st Street, Queens, NY 11101\n",
    "     # 21st Street Subway Station\n",
    "\n",
    "    \n",
    "# June pickup address: 107 East 31st Street, New York, NY 10016\n",
    "    # This is the address of a condo building which has a winery in the top floor\n",
    "    # https://www.vetrony.com/\n",
    "    # https://wineberry.com/contact/\n",
    "# June dropoff address: 4662 21st Street, Queens, NY 11101\n",
    "     # 21st Street Subway Station\n",
    "\n",
    "    \n",
    "# July pickup address: 107 East 31st Street, New York, NY 10016\n",
    "    # The condo/winery\n",
    "# July dropoff address: 4738 5th Street, Long Island City, NY 11101\n",
    "    # The 47th Avenue Garage\n",
    "\n",
    "    \n",
    "# August pickup address: 3628 14th Street, Astoria, NY 11106\n",
    "    # A car parts shop near a school\n",
    "# August dropoff address: 1310 Madison Avenue, New York, NY 10128\n",
    "    # The actual address is a women's clothing boutique \n",
    "    # But it's also close to a parking garage and a bus stop\n",
    "\n",
    "    \n",
    "# September pickup address: 3628 14th Street, Astoria, NY 11106\n",
    "    # A car parts shop near a school\n",
    "# September dropoff address: 1310 Madison Avenue, New York, NY 10128\n",
    "    # The actual address is a women's clothing boutique \n",
    "    # But it's also close to a parking garage and a bus stop\n",
    "\n",
    "    \n",
    "# October pickup address: 1945 Broadway, New York, NY 10023\n",
    "    # Juilliard, near a subway station\n",
    "# October dropoff address: 1310 Madison Avenue, New York, NY 10128\n",
    "    # The actual address is a women's clothing boutique \n",
    "    # But it's also close to a parking garage and a bus stop\n",
    "\n",
    "    \n",
    "# November pickup address: 1960 Broadway, New York, NY 10023\n",
    "    # Juilliard, near a subway station\n",
    "# November dropoff address: 1310 Madison Avenue, New York, NY 10128\n",
    "    # The actual address is a women's clothing boutique \n",
    "    # But it's also close to a parking garage and a bus stop\n",
    "\n",
    "    \n",
    "# December pickup address: 106 East 31st Street, New York, NY 10016\n",
    "    # This is across the street from the condo/winery\n",
    "    # It's the address of a travel agency\n",
    "# December dropoff address: 102 East 31st Street, New York, NY 10016\n",
    "    # In the same neighborhood as the condo/winery.\n",
    "\n",
    "\n",
    "# While some of the results make sense, like the subway stations and garages,\n",
    "# others are rather strange, like the condo/winery and others in that same\n",
    "# neighborhood. All of these strange locations are in the NoMad neighborhood of Manhattan\n",
    "# https://en.wikipedia.org/wiki/NoMad,_Manhattan\n",
    "# The NoMad neighborhood is fairly famous, and it is possible that people take\n",
    "# taxis to be tourists and see the sights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution: implement a data science solution to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the idea of your solution to the problem in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write codes to implement the solution in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "daily_trips = pd.read_csv('daily_trips_with_location_id.csv')\n",
    "election_results = pd.read_csv('election_results_by_taxi_zone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location id 1.0 has no voter information.\n",
      "Location id 57.0 has no voter information.\n",
      "Location id 105.0 has no voter information.\n",
      "Location id 199.0 has no voter information.\n",
      "Location id 253.0 has no voter information.\n",
      "Location id 264.0 has no voter information.\n",
      "Location id 265.0 has no voter information.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the features to use for predicting trump voter percent\n",
    "# Features:\n",
    "# Fraction of rides that are yellow in 2015\n",
    "# Fraction of rides that are green in 2015\n",
    "# Fraction of rides that are lyft in 2015\n",
    "# Fraction of rides that are other in 2015\n",
    "# Fraction of rides that are uber in 2015\n",
    "# Fraction of rides that are via in 2015\n",
    "# Total rides in 2015\n",
    "# Fraction of Trump voters in 2016 (this is the target variable)\n",
    "\n",
    "feature_lists = []\n",
    "car_types = daily_trips['car_type'].unique()\n",
    "car_type_num = len(car_types)\n",
    "car_type_dict = dict(zip(car_types,list(range(car_type_num))))\n",
    "for location_id,location in daily_trips[daily_trips['date'].str.contains('2015')].groupby('pickup_location_id'):\n",
    "    feature_list = [0]*(car_type_num+3)\n",
    "    type_counts = location.groupby('car_type')['trips'].count()\n",
    "    indicies = [car_type_dict[i] for i in type_counts.index.values]\n",
    "    values = [v for v in type_counts.values]\n",
    "    feature_list[-3] = np.sum(type_counts.values)\n",
    "    for i,v in zip(indicies,values):\n",
    "        feature_list[i] = v/feature_list[-3]\n",
    "    try:\n",
    "        feature_list[-2] = election_results.loc[election_results['locationid'] == location_id, 'trump'].iloc[0]>=0.5\n",
    "        feature_list[-1] = election_results.loc[election_results['locationid'] == location_id, 'trump'].iloc[0]<0.5\n",
    "        feature_lists.append(feature_list)\n",
    "    except:\n",
    "        print('Location id '+str(location_id)+' has no voter information.')\n",
    "        \n",
    "df = pd.DataFrame(feature_lists)\n",
    "#print(df)\n",
    "df.to_csv('feature_lists.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alexander\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'list'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1203\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1204\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    715\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-049d17074d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Create and train the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    159\u001b[0m                         \u001b[0mbatch_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                         name=layer.name + '_input')\n\u001b[0m\u001b[0;32m    162\u001b[0m                     \u001b[1;31m# This will build the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[1;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[0;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                              input_tensor=tensor)\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[1;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[0;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                                          name=self.name)\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   2141\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   2142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2143\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   6257\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6258\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6259\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6260\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   6261\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'list'."
     ]
    }
   ],
   "source": [
    "# Prepare the data for the neural network\n",
    "trn_data,val_tst_data = train_test_split(feature_lists,train_size=0.6,stratify=np.array(feature_lists)[:,-1])\n",
    "val_data,tst_data = train_test_split(val_tst_data,train_size=0.5,stratify=np.array(val_tst_data)[:,-1])\n",
    "\n",
    "trn_data = np.array(trn_data)\n",
    "val_data = np.array(val_data)\n",
    "tst_data = np.array(tst_data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "trn_data[:,:-2] = scaler.fit_transform(trn_data[:,:-2])\n",
    "val_data[:,:-2] = scaler.transform(val_data[:,:-2])\n",
    "tst_data[:,:-2] = scaler.transform(tst_data[:,:-2])\n",
    "\n",
    "# Create and train the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='sigmoid', input_shape=(features,)))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=False)]\n",
    "model.fit(trn_data[:,:-2],trn_data[:,-2:],validation_data=(val_data[:,:-2],val_data[:,-2:]),epochs=10000,callbacks=callbacks,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation confusion matrix:\n",
      "[[45  0]\n",
      " [ 4  2]]\n",
      "testing confusion matrix:\n",
      "[[46  0]\n",
      " [ 2  4]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "output = model.predict(val_data[:,:-2])[:,-2] >= threshold\n",
    "target = val_data[:,-2] >= threshold\n",
    "print('validation confusion matrix:')\n",
    "print(confusion_matrix(target,output))\n",
    "output = model.predict(tst_data[:,:-2])[:,-2] >= threshold\n",
    "target = tst_data[:,-2] >= threshold\n",
    "print('testing confusion matrix:')\n",
    "print(confusion_matrix(target,output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yellow', 'green', 'uber', 'gett', 'juno', 'lyft', 'other', 'via', 'volume', 'trump', 'volume', 'trump', 'volume', 'trump']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-7bd4035dea6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtst_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrn_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "## Alternate Model\n",
    "# Prepare the data for the logistic regression\n",
    "data = pd.read_csv('feature_lists.csv')\n",
    "#print(data)\n",
    "car_types = list(car_types)\n",
    "car_types.append('volume')\n",
    "car_types.append('trump')\n",
    "print(car_types)\n",
    "\n",
    "trn_data,tst_data = train_test_split(data,train_size=0.75,stratify=np.array(feature_lists)[:,-1])\n",
    " \n",
    "trn_y = trn_data[-1]\n",
    "print(trn_y)\n",
    "del trn_data[-1]\n",
    "\n",
    "tst_y = tst_data[-1]\n",
    "del tst_data[-1]\n",
    "\n",
    "# Logistic Regression Model\n",
    "logis = LogisticRegression()\n",
    "logis.fit(trn_data, trn_y)\n",
    "\n",
    "model_out = logis.predict(tst_data)\n",
    "metric = metric(trn_y, tst_y)\n",
    "\n",
    "# Alt model confusion matrix\n",
    "threshold = 0.5\n",
    "output = model.predict(val_data[:,:-2])[:,-2] >= threshold\n",
    "target = val_data[:,-2] >= threshold\n",
    "print('validation confusion matrix:')\n",
    "print(confusion_matrix(target,output))\n",
    "output = model.predict(tst_data[:,:-2])[:,-2] >= threshold\n",
    "target = tst_data[:,-2] >= threshold\n",
    "print('testing confusion matrix:')\n",
    "print(confusion_matrix(target,output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures, tables, or videos to communicate the results with the audience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 7 minutes' talk) to present about the case study . Each team present their case studies in class for 7 minutes.\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "**How to submit:**\n",
    "\n",
    "        Please submit through Canvas, in the Assignment \"Case Study 3\".\n",
    "        \n",
    "**Note: Each team only needs to submit one submission in Canvas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Peer-Review Grading Template:\n",
    "\n",
    "**Total Points: (100 points)** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "\n",
    "Please add an \"**X**\" mark in front of your rating: \n",
    "\n",
    "For example:\n",
    "\n",
    "*2: bad*\n",
    "          \n",
    "**X** *3: good*\n",
    "    \n",
    "*4: perfect*\n",
    "\n",
    "\n",
    "    ---------------------------------\n",
    "    The Problem: \n",
    "    ---------------------------------\n",
    "    \n",
    "    1. (10 points) how well did the team describe the problem they are trying to solve using the data? \n",
    "       0: not clear\n",
    "       2: I can barely understand the problem\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "    \n",
    "    2. (10 points) do you think the problem is important or has a potential impact?\n",
    "        0: not important at all\n",
    "        2: not sure if it is important\n",
    "        4: seems important, but not clear\n",
    "        6: interesting problem\n",
    "        8: an important problem, which I want to know the answer myself\n",
    "       10: very important, I would be happy invest money on a project like this.\n",
    "    \n",
    "    ----------------------------------\n",
    "    Data Collection and Processing:\n",
    "    ----------------------------------\n",
    "    \n",
    "    3. (10 points) Do you think the data collected/processed are relevant and sufficient for solving the above problem? \n",
    "       0: not clear\n",
    "       2: I can barely understand what data they are trying to collect/process\n",
    "       4: I can barely understand why the data is relevant to the problem\n",
    "       6: the data are relevant to the problem, but better data can be collected\n",
    "       8: the data collected are relevant and at a proper scale\n",
    "      10: the data are properly collected and they are sufficient\n",
    "\n",
    "    -----------------------------------\n",
    "    Data Exploration:\n",
    "    -----------------------------------\n",
    "    4. How well did the team solve the following task:\n",
    "    \n",
    "    (1) plot the spatial distribution of the pickup locations of 5000 Uber trips (10 points):\n",
    "       0: missing answer\n",
    "       4: okay, but with major problems\n",
    "       7: good, but with minor problems\n",
    "      10: perfect\n",
    "    \n",
    "\n",
    "    -----------------------------------\n",
    "    The Solution\n",
    "    -----------------------------------\n",
    "    5.  how well did the team describe the solution they used to solve the problem? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "       \n",
    "    6. how well is the solution in solving the problem? (10 points)\n",
    "       0: not relevant\n",
    "       2: barely relevant to the problem\n",
    "       4: okay solution, but there is an easier solution.\n",
    "       6: good, but can be improved\n",
    "       8: very good, but solution is simple/old\n",
    "       10: innovative and technically sound\n",
    "       \n",
    "    7. how well did the team implement the solution in python? (10 points)\n",
    "       0: the code is not relevant to the solution proposed\n",
    "       2: the code is barely understandable, but not relevant\n",
    "       4: okay, the code is clear but incorrect\n",
    "       6: good, the code is correct, but with major errors\n",
    "       8: very good, the code is correct, but with minor errors\n",
    "      10: perfect \n",
    "   \n",
    "    -----------------------------------\n",
    "    The Results\n",
    "    -----------------------------------\n",
    "     8.  How well did the team present the results they found in the data? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "      10: crystal clear\n",
    "       \n",
    "     9.  How do you think of the results they found in the data?  (5 points)\n",
    "       0: not clear\n",
    "       1: likely to be wrong\n",
    "       2: okay, maybe wrong\n",
    "       3: good, but can be improved\n",
    "       4: make sense, but not interesting\n",
    "       5: make sense and very interesting\n",
    "     \n",
    "    -----------------------------------\n",
    "    The Presentation\n",
    "    -----------------------------------\n",
    "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
    "       0: they are irrelevant\n",
    "       1: I can barely understand how they are related to each other\n",
    "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
    "       3: good, but the results don't make much sense in the context\n",
    "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
    "       5: a perfect story\n",
    "      \n",
    "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
    "       0: the team didn't present\n",
    "       1: bad, barely finished a small part of the talk\n",
    "       2: okay, barely finished most parts of the talk.\n",
    "       3: good, finished all parts of the talk, but some part is rushed\n",
    "       4: very good, but the allocation of time on different parts can be improved.\n",
    "       5: perfect timing and good use of time      \n",
    "\n",
    "    12. How well do you think of the presentation (overall quality)?  \n",
    "       0: the team didn't present\n",
    "       1: bad\n",
    "       2: okay\n",
    "       3: good\n",
    "       4: very good\n",
    "       5: perfect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
